{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6440da",
   "metadata": {},
   "source": [
    "#### Setting up game configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc45c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random # for random actions\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Open AI Gym dependencies\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box # for random actions and nxm for random observation space (frames)\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4a6d9",
   "metadata": {},
   "source": [
    "### Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vizdoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render_mode = False): # By default, rendering is disabled\n",
    "        # Inheriting from the Env class\n",
    "        super().__init__()\n",
    "        # Setup game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(\"github_vizdoom_repo/ViZDoom/scenarios/basic.cfg\") # Loading game configuration file\n",
    "\n",
    "        # Rendering mode : if unabled, the game will not be displayed but the training will be faster\n",
    "        if render_mode == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # In order to get the game frame size, run a dummy demo and get the screen buffer shape  with game.get_state().screen_buffer.shape\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype = np.uint8)\n",
    "        # Action space\n",
    "        self.action_space = Discrete(3) # left, right, shoot\n",
    "        self.current_step = 0\n",
    "\n",
    "\n",
    "    # Defining how to make a step in the env\n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8) # Possible actions\n",
    "        reward = self.game.make_action(actions[action], 4) # Defyining the frame skip parameter to 4\n",
    "\n",
    "        # Check if the frames are over\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            obs = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            terminated = self.game.is_episode_finished()\n",
    "            truncated = self.current_step >= 2100  # Max steps\n",
    "            info = {\"ammo\": ammo}\n",
    "        else: # Default zeros observation\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            info = {} # Empty info: no 0 beacuse the API doesn't allow it\n",
    "\n",
    "        self.current_step += 1\n",
    "        return obs, reward, terminated, truncated, info # Changed parameters order according to Gymnasium API\n",
    "    \n",
    "    def render():\n",
    "        pass\n",
    "    \n",
    "    # What appens when starting a new episode\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        obs = self.grayscale(state)\n",
    "        info = {\"ammo\": self.game.get_state().game_variables[0]}\n",
    "        self.current_step = 0\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    # Grayscale and resize the frames in order to reduce the observation space\n",
    "    ## POSSIBLE IMPROVEMENT : CUT OFF BOTTOM PART OF THE IMAGE WHERE THERE IS NO USEFUL INFORMATION\n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) # moveaxis moves the first element (0) to last position (-1)\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160, 1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62532fc9",
   "metadata": {},
   "source": [
    "### Setting up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}.zip'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf49e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path = CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d142340",
   "metadata": {},
   "source": [
    "### Training the agent with the PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym() # non rendered-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90571c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate PPO model\n",
    "model = PPO('CnnPolicy', # policy type -> CnnPolicy since we are working on image frames\n",
    "            env,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=1,\n",
    "            learning_rate=0.00025,\n",
    "            n_steps=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = 100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f072c",
   "metadata": {},
   "source": [
    "### Testing the trained agent on real-time game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6477fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation policy to test the agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4536135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best model from disk\n",
    "model = PPO.load(\"./train/train_basic/best_model_100000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render_mode= True) # rendered-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action[0])\n",
    "        time.sleep(0.05)\n",
    "        total_reward  += reward\n",
    "\n",
    "    print(f\"Total Reward for Episode {episode} is {total_reward}\")\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
